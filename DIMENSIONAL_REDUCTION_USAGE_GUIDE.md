# Servicio de Reducci√≥n Dimensional - Gu√≠a de Uso Actualizada

## Cambios Realizados

### ‚úÖ Problemas Corregidos

1. **Normalizaci√≥n Final**: Reemplazada normalizaci√≥n r√≠gida [-1,1] por z-score que preserva la estructura de clusters
2. **PCA Mejorado**: Mayor precisi√≥n de convergencia, deflaci√≥n robusta con verificaci√≥n de simetr√≠a
3. **t-SNE Optimizado**: Inicializaci√≥n con PCA, momentum, early exaggeration, 1000 iteraciones
4. **Combinaci√≥n de Embeddings**: M√©todos para combinar m√∫ltiples tipos de embeddings con pesos adaptativos
5. **Validaci√≥n y Diagn√≥stico**: Detecci√≥n autom√°tica de problemas de varianza y recomendaciones

### üîÑ API Actualizada

#### M√©todos Principales

```typescript
// Reducci√≥n b√°sica (con z-score normalizaci√≥n)
service.reduceDimensionality(vectors, 'tsne', 3)

// Reducci√≥n con validaci√≥n autom√°tica
service.reduceDimensionalityWithValidation(vectors, 'tsne', 3)

// Combinaci√≥n de m√∫ltiples embeddings
service.reduceDimensionalityWithCombinedEmbeddings(
  vectorsWithMultipleEmbeddings,
  { cultural: 0.4, narrative: 0.6 },
  'tsne',
  3
)
```

#### M√©todos Especializados

```typescript
// Para embeddings CLIP (soporta combinaci√≥n)
service.reduce3DForCLIP(vectors)

// Para embeddings de texto (soporta combinaci√≥n)
service.reduce3DForText(vectors)

// Para embeddings culturales combinados
service.reduce3DForCultural(vectors, 'narrative', 0.5)
```

## Casos de Uso Recomendados

### 1. Embeddings Culturales (Baja Varianza)

**Problema**: Los embeddings culturales tienen poca varianza ‚Üí proyecci√≥n concentrada
**Soluci√≥n**: Combinar con otros embeddings

```typescript
// Opci√≥n A: Combinar cultural + narrativa
const combinedVectors = photos.map((photo) => ({
  id: photo.id,
  embeddings: {
    cultural: photo.culturalEmbedding,
    narrative: photo.narrativeEmbedding,
  },
}))

const result = await service.reduce3DForCultural(
  combinedVectors,
  'narrative', // Partner preferido
  0.4 // Peso cultural (40%)
)

// Opci√≥n B: Combinar cultural + CLIP
const result2 = await service.reduceDimensionalityWithCombinedEmbeddings(
  combinedVectors,
  { cultural: 0.3, clip: 0.7 }, // Mayor peso a CLIP
  'tsne',
  3
)
```

### 2. Validaci√≥n Previa y Diagn√≥stico

```typescript
// Validar calidad de embeddings antes de procesar
const { result, diagnostics } = await service.reduceDimensionalityWithValidation(vectors, 'tsne', 3)

console.log('Issues found:', diagnostics.issues)
console.log('Recommendations:', diagnostics.recommendations)
console.log('Mean variance:', diagnostics.statistics.meanVariance)

if (diagnostics.statistics.hasLowVarianceComponents) {
  console.log('‚ö†Ô∏è Low variance detected - consider combining embeddings')
}
```

### 3. Embeddings CLIP con Alta Varianza

```typescript
// Para embeddings CLIP puros (ya tienen buena varianza)
const clipVectors = photos.map((photo) => ({
  id: photo.id,
  embedding: photo.clipEmbedding,
}))

const result = await service.reduce3DForCLIP(clipVectors)
```

## Interpretaci√≥n de Resultados

### ‚ùå Interpretaci√≥n Incorrecta

```
"El eje X representa el aspecto cultural"
"Valores altos en Y = m√°s art√≠stico"
```

### ‚úÖ Interpretaci√≥n Correcta

```
- Puntos cercanos = fotos similares
- Clusters = grupos de fotos con caracter√≠sticas similares
- Distancia relativa = medida de similitud
- Dispersi√≥n general = varianza en el dataset
```

## Par√°metros de Configuraci√≥n

### t-SNE Mejorado

- **Iteraciones**: 1000 (vs 200 anterior)
- **Early Exaggeration**: 250 iteraciones con factor 4.0
- **Momentum**: 0.5 ‚Üí 0.8 gradualmente
- **Learning Rate**: Adaptativo (500 ‚Üí 200 ‚Üí 100)
- **Inicializaci√≥n**: PCA en lugar de aleatoria

### PCA Mejorado

- **Tolerancia**: 1e-8 (vs 1e-6 anterior)
- **Iteraciones**: 1000 (vs 100 anterior)
- **Deflaci√≥n**: Con verificaci√≥n de simetr√≠a
- **Convergencia**: Eigenvalue + eigenvector change

### Normalizaci√≥n

- **Anterior**: Min-max a [-1,1] ‚Üí ‚ùå Aplasta clusters
- **Nueva**: Z-score (Œº=0, œÉ=1) ‚Üí ‚úÖ Preserva estructura

## Troubleshooting

### Problema: "Puntos muy concentrados"

- **Causa**: Baja varianza en embeddings
- **Soluci√≥n**: Combinar con otros tipos de embeddings
- **M√©todo**: `service.combineCulturalWithOtherEmbeddings()`

### Problema: "Nube esf√©rica sin estructura"

- **Causa**: Normalizaci√≥n inadecuada, pocas iteraciones
- **Soluci√≥n**: Usar nueva implementaci√≥n con z-score y m√°s iteraciones
- **Verificaci√≥n**: `diagnostics.statistics.hasLowVarianceComponents`

### Problema: "Clusters no se separan"

- **Causa**: Embeddings demasiado similares o dimensiones dominantes
- **Soluci√≥n**: Validar con `validateAndDiagnoseEmbeddings()`
- **Alternativa**: Probar PCA en lugar de t-SNE

## Ejemplo Completo

```typescript
const service = new DimensionalReductionService()

// Caso: Embeddings culturales + narrativos
const photos = [
  {
    id: 1,
    culturalEmbedding: [0.1, 0.15, 0.12, ...], // Baja varianza
    narrativeEmbedding: [0.8, -0.3, 0.6, ...] // Alta varianza
  },
  // ... m√°s fotos
]

// 1. Preparar datos
const combinedVectors = photos.map(photo => ({
  id: photo.id,
  embeddings: {
    cultural: photo.culturalEmbedding,
    narrative: photo.narrativeEmbedding
  }
}))

// 2. Validar calidad
const diagnostics = service.validateAndDiagnoseEmbeddings(
  photos.map(p => ({ id: p.id, embedding: p.culturalEmbedding }))
)

if (diagnostics.statistics.hasLowVarianceComponents) {
  console.log('‚ö†Ô∏è Using combined embeddings due to low cultural variance')

  // 3. Reducir con combinaci√≥n optimizada
  const result = await service.reduce3DForCultural(
    combinedVectors,
    'narrative', // Partner que aporta varianza
    0.3 // Peso menor para cultural por su baja varianza
  )
} else {
  // 4. Usar embeddings culturales solos si tienen suficiente varianza
  const result = await service.reduceDimensionality(
    photos.map(p => ({ id: p.id, embedding: p.culturalEmbedding })),
    'tsne',
    3
  )
}

// 5. Usar coordenadas directamente en ThreeJS
result.forEach(item => {
  const [x, y, z] = item.coordinates
  scene.add(new THREE.Mesh(
    geometry,
    material.clone()
  ).position.set(x, y, z))
})
```

## Pr√≥ximos Pasos

1. **Testear** las proyecciones con PCA primero para validar dispersi√≥n
2. **Experimentar** con diferentes combinaciones de pesos
3. **Usar** el diagn√≥stico para optimizar par√°metros por dataset
4. **Evitar** normalizaciones adicionales en ThreeJS - usar coordenadas directamente
